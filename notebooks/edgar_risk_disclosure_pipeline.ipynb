{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR XBRL Parser: Risk Factors & Cybersecurity\n",
    "\n",
    "This notebook demonstrates how to download and parse XBRL files from SEC EDGAR, specifically extracting:\n",
    "- Section 1A: Risk Factors\n",
    "- Section 1C: Cybersecurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENAPI KEY - key removed, confidential\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-A165Dgv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\")[:15])  # shows first few chars only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key is active!\n",
      "Available models: ['gpt-4-0613', 'gpt-4', 'gpt-3.5-turbo', 'gpt-5.1-codex-mini', 'gpt-5.1-chat-latest']\n"
     ]
    }
   ],
   "source": [
    "# check if API key is active\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "try:\n",
    "    response = client.models.list()\n",
    "    print(\"✅ API key is active!\")\n",
    "    print(\"Available models:\", [m.id for m in response.data[:5]])\n",
    "except Exception as e:\n",
    "    print(\"❌ API key is invalid or inactive.\")\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sec-edgar-downloader in /opt/anaconda3/lib/python3.13/site-packages (5.0.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.13/site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.13/site-packages (5.3.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyrate-limiter>=3.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from sec-edgar-downloader) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sec-edgar-downloader requests beautifulsoup4 lxml pandas --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up your user agent and company information. **Important**: SEC requires you to identify yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Update with your information\n",
    "COMPANY_NAME = \"Your Company Name\"\n",
    "EMAIL = \"your.email@example.com\"\n",
    "\n",
    "# User agent for SEC requests\n",
    "HEADERS = {\n",
    "    'User-Agent': f'{COMPANY_NAME} {EMAIL}',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Host': 'www.sec.gov'\n",
    "}\n",
    "\n",
    "# Base URLs\n",
    "SEC_BASE_URL = \"https://www.sec.gov\"\n",
    "EDGAR_SEARCH_URL = \"https://www.sec.gov/cgi-bin/browse-edgar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_cik(ticker: str) -> str:\n",
    "    \"\"\"\n",
    "    Get CIK (Central Index Key) from ticker symbol.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={ticker}&type=10-K&dateb=&owner=exclude&count=1\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    cik_element = soup.find('span', class_='companyName')\n",
    "    \n",
    "    if cik_element:\n",
    "        cik_match = re.search(r'CIK=(\\d+)', str(cik_element))\n",
    "        if cik_match:\n",
    "            return cik_match.group(1).zfill(10)\n",
    "    \n",
    "    raise ValueError(f\"Could not find CIK for ticker: {ticker}\")\n",
    "\n",
    "\n",
    "def get_latest_10k_filing(cik: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Get the latest 10-K filing information for a company.\n",
    "    \"\"\"\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Find the most recent 10-K\n",
    "    filings = data['filings']['recent']\n",
    "    for i, form in enumerate(filings['form']):\n",
    "        if form == '10-K':\n",
    "            return {\n",
    "                'accessionNumber': filings['accessionNumber'][i],\n",
    "                'filingDate': filings['filingDate'][i],\n",
    "                'primaryDocument': filings['primaryDocument'][i]\n",
    "            }\n",
    "    \n",
    "    raise ValueError(f\"No 10-K filing found for CIK: {cik}\")\n",
    "\n",
    "\n",
    "def download_filing_files(cik: str, accession_number: str, output_dir: str = './filings') -> str:\n",
    "    \"\"\"\n",
    "    Download all files associated with a filing.\n",
    "    \"\"\"\n",
    "    # Remove dashes from accession number for URL\n",
    "    accession_no_dash = accession_number.replace('-', '')\n",
    "    \n",
    "    # Create output directory\n",
    "    filing_dir = Path(output_dir) / f\"{cik}_{accession_number}\"\n",
    "    filing_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the filing index page\n",
    "    index_url = f\"https://www.sec.gov/cgi-bin/viewer?action=view&cik={cik}&accession_number={accession_number}&xbrl_type=v\"\n",
    "    \n",
    "    # Alternative: Direct archive access\n",
    "    archive_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_dash}/\"\n",
    "    \n",
    "    print(f\"Downloading from: {archive_url}\")\n",
    "    response = requests.get(archive_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all file links\n",
    "    files_downloaded = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href and not href.startswith('?'):\n",
    "            file_url = f\"{SEC_BASE_URL}{href}\" if href.startswith('/') else f\"{archive_url}{href}\"\n",
    "            filename = href.split('/')[-1]\n",
    "            \n",
    "            # Download XML files and the main filing\n",
    "            if filename.endswith(('.xml', '.htm', '.html', '.xsd')):\n",
    "                print(f\"  Downloading: {filename}\")\n",
    "                file_response = requests.get(file_url, headers=HEADERS)\n",
    "                \n",
    "                file_path = filing_dir / filename\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(file_response.content)\n",
    "                \n",
    "                files_downloaded.append(str(file_path))\n",
    "    \n",
    "    print(f\"Downloaded {len(files_downloaded)} files to {filing_dir}\")\n",
    "    return str(filing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XBRL Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_xbrl_instance_file(filing_dir: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Find the main XBRL instance document (typically ends with _htm.xml).\n",
    "    \"\"\"\n",
    "    filing_path = Path(filing_dir)\n",
    "    \n",
    "    # Look for the instance document (usually the largest .xml file or one ending in _htm.xml)\n",
    "    xml_files = list(filing_path.glob('*.xml'))\n",
    "    \n",
    "    # Prioritize files ending with _htm.xml\n",
    "    for xml_file in xml_files:\n",
    "        if '_htm.xml' in xml_file.name:\n",
    "            return str(xml_file)\n",
    "    \n",
    "    # Otherwise, return the largest XML file\n",
    "    if xml_files:\n",
    "        largest_file = max(xml_files, key=lambda x: x.stat().st_size)\n",
    "        return str(largest_file)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_xbrl_for_text_blocks(xml_file: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse XBRL file and extract text blocks, especially Risk Factors and Cybersecurity.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Common XBRL namespaces\n",
    "    namespaces = {\n",
    "        'us-gaap': 'http://fasb.org/us-gaap/2023',\n",
    "        'dei': 'http://xbrl.sec.gov/dei/2023',\n",
    "        'xbrli': 'http://www.xbrl.org/2003/instance',\n",
    "    }\n",
    "    \n",
    "    # Try to detect namespaces dynamically\n",
    "    for elem in root.iter():\n",
    "        if '}' in elem.tag:\n",
    "            ns = elem.tag.split('}')[0].strip('{')\n",
    "            prefix = elem.tag.split('}')[1].split(':')[0] if ':' in elem.tag else None\n",
    "            if prefix and prefix not in namespaces:\n",
    "                namespaces[prefix] = ns\n",
    "    \n",
    "    text_blocks = {}\n",
    "    \n",
    "    # Target text block tags\n",
    "    target_tags = [\n",
    "        'RiskFactorTextBlock',\n",
    "        'RiskFactorsTextBlock', \n",
    "        'CybersecurityTextBlock',\n",
    "        'CybersecurityDisclosureTextBlock',\n",
    "        'BusinessDescriptionAndBasisOfPresentationTextBlock',\n",
    "        'ManagementDiscussionAndAnalysisTextBlock',\n",
    "    ]\n",
    "    \n",
    "    # Search through all elements\n",
    "    for elem in root.iter():\n",
    "        tag_name = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag\n",
    "        \n",
    "        if any(target in tag_name for target in target_tags):\n",
    "            text_content = elem.text\n",
    "            if text_content:\n",
    "                # Clean up HTML/XBRL formatting\n",
    "                text_content = clean_xbrl_text(text_content)\n",
    "                text_blocks[tag_name] = text_content\n",
    "                print(f\"Found: {tag_name} ({len(text_content)} characters)\")\n",
    "    \n",
    "    return text_blocks\n",
    "\n",
    "\n",
    "def clean_xbrl_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean XBRL text content by removing HTML tags and extra whitespace.\n",
    "    \"\"\"\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_htm_for_sections(filing_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the HTML filing document for specific sections.\n",
    "    This is a fallback if XBRL text blocks are not available.\n",
    "    \"\"\"\n",
    "    filing_path = Path(filing_dir)\n",
    "    htm_files = list(filing_path.glob('*.htm')) + list(filing_path.glob('*.html'))\n",
    "    \n",
    "    if not htm_files:\n",
    "        return {}\n",
    "    \n",
    "    # Usually the primary document is the largest\n",
    "    main_file = max(htm_files, key=lambda x: x.stat().st_size)\n",
    "    \n",
    "    with open(main_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    sections = {}\n",
    "    \n",
    "    # Look for Item 1A - Risk Factors\n",
    "    risk_pattern = re.compile(r'item\\s*1a[\\s\\.:-]*risk\\s*factors?', re.IGNORECASE)\n",
    "    cyber_pattern = re.compile(r'item\\s*1c[\\s\\.:-]*cybersecurity', re.IGNORECASE)\n",
    "    \n",
    "    # Find all text elements\n",
    "    all_text = soup.get_text()\n",
    "    \n",
    "    # Extract Item 1A\n",
    "    risk_match = risk_pattern.search(all_text)\n",
    "    if risk_match:\n",
    "        start_idx = risk_match.start()\n",
    "        # Find the next major section (Item 1B or Item 2)\n",
    "        next_section = re.search(r'item\\s*[12]b?[\\s\\.:-]', all_text[start_idx+50:], re.IGNORECASE)\n",
    "        if next_section:\n",
    "            end_idx = start_idx + 50 + next_section.start()\n",
    "            sections['RiskFactors'] = all_text[start_idx:end_idx].strip()\n",
    "    \n",
    "    # Extract Item 1C\n",
    "    cyber_match = cyber_pattern.search(all_text)\n",
    "    if cyber_match:\n",
    "        start_idx = cyber_match.start()\n",
    "        # Find the next major section (Item 1D or Item 2)\n",
    "        next_section = re.search(r'item\\s*[12][d]?[\\s\\.:-]', all_text[start_idx+50:], re.IGNORECASE)\n",
    "        if next_section:\n",
    "            end_idx = start_idx + 50 + next_section.start()\n",
    "            sections['Cybersecurity'] = all_text[start_idx:end_idx].strip()\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Execution - Download and Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests\n",
    "from pathlib import Path\n",
    "\n",
    "UA = \"Your Name your.email@example.com\"  # required by SEC\n",
    "SESS = requests.Session()\n",
    "SESS.headers.update({\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept\": \"application/json\",\n",
    "})\n",
    "\n",
    "def get_company_cik(ticker: str) -> str:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    r = SESS.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    for _, v in data.items():\n",
    "        if v[\"ticker\"].upper() == ticker.upper():\n",
    "            return str(v[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"Ticker {ticker} not found.\")\n",
    "\n",
    "def get_latest_10k_filing(cik10: str, max_retries: int = 3):\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik10}.json\"\n",
    "    for attempt in range(max_retries):\n",
    "        r = SESS.get(url, timeout=30)\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(0.6)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            print(\"Non-JSON response:\\n\", r.text[:500])\n",
    "            raise\n",
    "        forms = data[\"filings\"][\"recent\"]\n",
    "        for i, form in enumerate(forms[\"form\"]):\n",
    "            if form == \"10-K\":\n",
    "                acc = forms[\"accessionNumber\"][i]\n",
    "                return {\n",
    "                    \"accessionNumber\": acc,\n",
    "                    \"accession_no_dash\": acc.replace(\"-\", \"\"),\n",
    "                    \"filingDate\": forms[\"filingDate\"][i],\n",
    "                }\n",
    "    raise RuntimeError(\"No 10-K found or rate-limited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10-K for: TSLA\n",
      "==================================================\n",
      "\n",
      "1. Getting CIK...\n",
      "   CIK: 0001318605\n",
      "\n",
      "2. Getting latest 10-K filing...\n",
      "   Accession Number: 0001628280-25-003063\n",
      "   Filing Date: 2025-01-30\n",
      "\n",
      "3. Downloading filing files...\n",
      "Downloading from: https://www.sec.gov/Archives/edgar/data/0001318605/000162828025003063/\n",
      "  Downloading: index.htm\n",
      "  Downloading: search.htm\n",
      "  Downloading: howinvestigationswork.html\n",
      "  Downloading: brokers.htm\n",
      "  Downloading: quickedgar.htm\n",
      "  Downloading: companysearch.html\n",
      "  Downloading: secforms.htm\n",
      "  Downloading: publicdocs.htm\n",
      "  Downloading: index.html\n",
      "  Downloading: upcoming-events.htm\n",
      "  Downloading: 0001628280-25-003063-index-headers.html\n",
      "  Downloading: 0001628280-25-003063-index.html\n",
      "  Downloading: ex41.htm\n",
      "  Downloading: FilingSummary.xml\n",
      "  Downloading: R1.htm\n",
      "  Downloading: R10.htm\n",
      "  Downloading: R11.htm\n",
      "  Downloading: R12.htm\n",
      "  Downloading: R13.htm\n",
      "  Downloading: R14.htm\n",
      "  Downloading: R15.htm\n",
      "  Downloading: R16.htm\n",
      "  Downloading: R17.htm\n",
      "  Downloading: R18.htm\n",
      "  Downloading: R19.htm\n",
      "  Downloading: R2.htm\n",
      "  Downloading: R20.htm\n",
      "  Downloading: R21.htm\n",
      "  Downloading: R22.htm\n",
      "  Downloading: R23.htm\n",
      "  Downloading: R24.htm\n",
      "  Downloading: R25.htm\n",
      "  Downloading: R26.htm\n",
      "  Downloading: R27.htm\n",
      "  Downloading: R28.htm\n",
      "  Downloading: R29.htm\n",
      "  Downloading: R3.htm\n",
      "  Downloading: R30.htm\n",
      "  Downloading: R31.htm\n",
      "  Downloading: R32.htm\n",
      "  Downloading: R33.htm\n",
      "  Downloading: R34.htm\n",
      "  Downloading: R35.htm\n",
      "  Downloading: R36.htm\n",
      "  Downloading: R37.htm\n",
      "  Downloading: R38.htm\n",
      "  Downloading: R39.htm\n",
      "  Downloading: R4.htm\n",
      "  Downloading: R40.htm\n",
      "  Downloading: R41.htm\n",
      "  Downloading: R42.htm\n",
      "  Downloading: R43.htm\n",
      "  Downloading: R44.htm\n",
      "  Downloading: R45.htm\n",
      "  Downloading: R46.htm\n",
      "  Downloading: R47.htm\n",
      "  Downloading: R48.htm\n",
      "  Downloading: R49.htm\n",
      "  Downloading: R5.htm\n",
      "  Downloading: R50.htm\n",
      "  Downloading: R51.htm\n",
      "  Downloading: R52.htm\n",
      "  Downloading: R53.htm\n",
      "  Downloading: R54.htm\n",
      "  Downloading: R55.htm\n",
      "  Downloading: R56.htm\n",
      "  Downloading: R57.htm\n",
      "  Downloading: R58.htm\n",
      "  Downloading: R59.htm\n",
      "  Downloading: R6.htm\n",
      "  Downloading: R60.htm\n",
      "  Downloading: R61.htm\n",
      "  Downloading: R62.htm\n",
      "  Downloading: R63.htm\n",
      "  Downloading: R64.htm\n",
      "  Downloading: R65.htm\n",
      "  Downloading: R66.htm\n",
      "  Downloading: R67.htm\n",
      "  Downloading: R68.htm\n",
      "  Downloading: R69.htm\n",
      "  Downloading: R7.htm\n",
      "  Downloading: R70.htm\n",
      "  Downloading: R71.htm\n",
      "  Downloading: R72.htm\n",
      "  Downloading: R73.htm\n",
      "  Downloading: R74.htm\n",
      "  Downloading: R75.htm\n",
      "  Downloading: R76.htm\n",
      "  Downloading: R77.htm\n",
      "  Downloading: R78.htm\n",
      "  Downloading: R79.htm\n",
      "  Downloading: R8.htm\n",
      "  Downloading: R80.htm\n",
      "  Downloading: R81.htm\n",
      "  Downloading: R82.htm\n",
      "  Downloading: R83.htm\n",
      "  Downloading: R84.htm\n",
      "  Downloading: R85.htm\n",
      "  Downloading: R86.htm\n",
      "  Downloading: R87.htm\n",
      "  Downloading: R88.htm\n",
      "  Downloading: R89.htm\n",
      "  Downloading: R9.htm\n",
      "  Downloading: R90.htm\n",
      "  Downloading: R91.htm\n",
      "  Downloading: R92.htm\n",
      "  Downloading: R93.htm\n",
      "  Downloading: R94.htm\n",
      "  Downloading: R95.htm\n",
      "  Downloading: tsla-20241231.htm\n",
      "  Downloading: tsla-20241231.xsd\n",
      "  Downloading: tsla-20241231_cal.xml\n",
      "  Downloading: tsla-20241231_def.xml\n",
      "  Downloading: tsla-20241231_htm.xml\n",
      "  Downloading: tsla-20241231_lab.xml\n",
      "  Downloading: tsla-20241231_pre.xml\n",
      "  Downloading: tsla-2024x12x31xex19.htm\n",
      "  Downloading: tsla-2024x12x31xex211.htm\n",
      "  Downloading: tsla-2024x12x31xex231.htm\n",
      "  Downloading: tsla-2024x12x31xex311.htm\n",
      "  Downloading: tsla-2024x12x31xex312.htm\n",
      "  Downloading: tsla-2024x12x31xex321.htm\n",
      "  Downloading: tsla-2024x12x31xex472.htm\n",
      "  Downloading: sec_access.htm\n",
      "  Downloading: oacq.htm\n",
      "  Downloading: privacy.htm\n",
      "Downloaded 126 files to filings/0001318605_0001628280-25-003063\n",
      "   Files saved to: filings/0001318605_0001628280-25-003063\n"
     ]
    }
   ],
   "source": [
    "# Specify the company ticker\n",
    "TICKER = \"TSLA\"  # Change this to any ticker you want\n",
    "\n",
    "print(f\"Processing 10-K for: {TICKER}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Get CIK\n",
    "print(\"\\n1. Getting CIK...\")\n",
    "cik = get_company_cik(TICKER)\n",
    "print(f\"   CIK: {cik}\")\n",
    "\n",
    "# Step 2: Get latest 10-K filing\n",
    "print(\"\\n2. Getting latest 10-K filing...\")\n",
    "filing_info = get_latest_10k_filing(cik)\n",
    "print(f\"   Accession Number: {filing_info['accessionNumber']}\")\n",
    "print(f\"   Filing Date: {filing_info['filingDate']}\")\n",
    "\n",
    "# Step 3: Download filing files\n",
    "print(\"\\n3. Downloading filing files...\")\n",
    "filing_dir = download_filing_files(cik, filing_info['accessionNumber'])\n",
    "print(f\"   Files saved to: {filing_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Text Blocks from XBRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Parsing XBRL file...\n",
      "   Found XBRL file: tsla-20241231_htm.xml\n",
      "\n",
      "5. Trying HTML parsing as fallback...\n",
      "\n",
      "Extracted 2 text sections\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4. Parsing XBRL file...\")\n",
    "xbrl_file = find_xbrl_instance_file(filing_dir)\n",
    "\n",
    "if xbrl_file:\n",
    "    print(f\"   Found XBRL file: {Path(xbrl_file).name}\")\n",
    "    text_blocks = parse_xbrl_for_text_blocks(xbrl_file)\n",
    "else:\n",
    "    print(\"   No XBRL instance file found.\")\n",
    "    text_blocks = {}\n",
    "\n",
    "# If XBRL doesn't have text blocks, try HTML parsing\n",
    "if not text_blocks:\n",
    "    print(\"\\n5. Trying HTML parsing as fallback...\")\n",
    "    text_blocks = parse_htm_for_sections(filing_dir)\n",
    "\n",
    "print(f\"\\nExtracted {len(text_blocks)} text sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Section: RiskFactors\n",
      "Length: 80 characters\n",
      "================================================================================\n",
      "Item 1A.Risk Factors13Item 1B.Unresolved Staff Comments27Item 1C.Cybersecurity28\n",
      "\n",
      "================================================================================\n",
      "Section: Cybersecurity\n",
      "Length: 3304 characters\n",
      "================================================================================\n",
      "Item 1C.Cybersecurity28Item 2.Properties29Item 3.Legal Proceedings29Item 4.Mine Safety Disclosures29 PART II. Item 5.Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities30Item 6.[Reserved]31Item 7.Management's Discussion and Analysis of Financial Condition and Results of Operations32Item 7A.Quantitative and Qualitative Disclosures about Market Risk44Item 8.Financial Statements and Supplementary Data45Item 9.Changes in and Disagreements with Accountants on Accounting and Financial Disclosure91Item 9A.Controls and Procedures91Item 9B.Other Information92Item 9C.Disclosure Regarding Foreign Jurisdictions that Prevent Inspections92 PART III. Item 10.Directors, Executive Officers and Corporate Governance93Item 11.Executive Compensation93Item 12.Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters93Item 13.Certain Relationships and Related Transactions, and Director Independence93Item 14.Principa\n",
      "\n",
      "... [truncated] ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display summaries of extracted sections\n",
    "for section_name, content in text_blocks.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Section: {section_name}\")\n",
    "    print(f\"Length: {len(content)} characters\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(content[:1000])  # Show first 1000 characters\n",
    "    if len(content) > 1000:\n",
    "        print(\"\\n... [truncated] ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Extracted Sections to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/brucewayne/Documents/extracted_sections/RiskFactors.txt\n",
      "Saved: /Users/brucewayne/Documents/extracted_sections/Cybersecurity.txt\n",
      "\n",
      "All sections saved to JSON: /Users/brucewayne/Documents/extracted_sections/all_sections.json\n"
     ]
    }
   ],
   "source": [
    "# 8. Save Extracted Sections to Files\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "# make a safe filename from a section title\n",
    "def _safe(name: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', name).strip('_')[:80]\n",
    "\n",
    "# root output dir\n",
    "output_dir = Path(\"/Users/brucewayne/Documents\") / \"extracted_sections\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save each section as its own .txt\n",
    "for section_name, content in text_blocks.items():\n",
    "    output_file = output_dir / f\"{_safe(section_name)}.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Saved: {output_file}\")\n",
    "\n",
    "# also save a single JSON with all sections + a little metadata\n",
    "json_file = output_dir / \"all_sections.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"ticker\": TICKER,\n",
    "            \"cik\": cik,\n",
    "            \"filing_date\": filing_info[\"filingDate\"],\n",
    "            \"accession_number\": filing_info[\"accessionNumber\"],\n",
    "            \"sections\": text_blocks,   # dict of {section: text}\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "        ensure_ascii=False,\n",
    "    )\n",
    "\n",
    "print(f\"\\nAll sections saved to JSON: {json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Extracted Sections:\n",
      "         Section  Character Count  Word Count                                                                                                                                                                                                      Preview\n",
      "0    RiskFactors               80           7                                                                                                                             Item 1A.Risk Factors13Item 1B.Unresolved Staff Comments27Item 1C.Cybersecurity28\n",
      "1  Cybersecurity             3304         417  Item 1C.Cybersecurity28Item 2.Properties29Item 3.Legal Proceedings29Item 4.Mine Safety Disclosures29 PART II. Item 5.Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purch...\n",
      "\n",
      "Summary saved to: /Users/brucewayne/Documents/extracted_sections/summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a summary dataframe\n",
    "summary_data = []\n",
    "for section_name, content in text_blocks.items():\n",
    "    summary_data.append({\n",
    "        'Section': section_name,\n",
    "        'Character Count': len(content),\n",
    "        'Word Count': len(content.split()),\n",
    "        'Preview': content[:200] + '...' if len(content) > 200 else content\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary of Extracted Sections:\")\n",
    "print(df_summary.to_string())\n",
    "\n",
    "# Save summary to CSV\n",
    "csv_file = output_dir / \"summary.csv\"\n",
    "df_summary.to_csv(csv_file, index=False)\n",
    "print(f\"\\nSummary saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Chunk Text for RAG\n",
    "\n",
    "Split the extracted sections into chunks suitable for RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 6 chunks from 2 sections\n",
      "         section  chunk_id                                               text  \\\n",
      "0    RiskFactors         0  Item 1A.Risk Factors13Item 1B.Unresolved Staff...   \n",
      "1  Cybersecurity         0  Item 1C.Cybersecurity28Item 2.Properties29Item...   \n",
      "2  Cybersecurity         1  m 12.Security Ownership of Certain Beneficial ...   \n",
      "3  Cybersecurity         2  cerning supply chain constraints, our strategy...   \n",
      "4  Cybersecurity         3  tatements and you should not place undue relia...   \n",
      "\n",
      "   char_count ticker filing_date  \n",
      "0          80   TSLA  2025-01-30  \n",
      "1        1000   TSLA  2025-01-30  \n",
      "2        1000   TSLA  2025-01-30  \n",
      "3        1000   TSLA  2025-01-30  \n",
      "4         904   TSLA  2025-01-30  \n",
      "\n",
      "Chunks saved to: /Users/brucewayne/Documents/extracted_sections/chunks.json\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Chunk each section\n",
    "all_chunks = []\n",
    "for section_name, content in text_blocks.items():\n",
    "    chunks = chunk_text(content, chunk_size=1000, overlap=200)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            'section': section_name,\n",
    "            'chunk_id': i,\n",
    "            'text': chunk,\n",
    "            'char_count': len(chunk),\n",
    "            'ticker': TICKER,\n",
    "            'filing_date': filing_info['filingDate']\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(all_chunks)\n",
    "print(f\"\\nCreated {len(all_chunks)} chunks from {len(text_blocks)} sections\")\n",
    "print(df_chunks.head())\n",
    "\n",
    "# Save chunks\n",
    "chunks_file = output_dir / \"chunks.json\"\n",
    "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, indent=2)\n",
    "\n",
    "print(f\"\\nChunks saved to: {chunks_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing Multiple Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing: AAPL\n",
      "================================================================================\n",
      "CIK: 0000320193\n",
      "Filing Date: 2025-10-31\n",
      "Downloading from: https://www.sec.gov/Archives/edgar/data/0000320193/000032019325000079/\n",
      "  Downloading: index.htm\n",
      "  Downloading: search.htm\n",
      "  Downloading: howinvestigationswork.html\n",
      "  Downloading: brokers.htm\n",
      "  Downloading: quickedgar.htm\n",
      "  Downloading: companysearch.html\n",
      "  Downloading: secforms.htm\n",
      "  Downloading: publicdocs.htm\n",
      "  Downloading: index.html\n",
      "  Downloading: upcoming-events.htm\n",
      "  Downloading: 0000320193-25-000079-index-headers.html\n",
      "  Downloading: 0000320193-25-000079-index.html\n",
      "  Downloading: a10-kexhibit21109272025.htm\n",
      "  Downloading: a10-kexhibit23109272025.htm\n",
      "  Downloading: a10-kexhibit31109272025.htm\n",
      "  Downloading: a10-kexhibit31209272025.htm\n",
      "  Downloading: a10-kexhibit32109272025.htm\n",
      "  Downloading: a10-kexhibit4109272025.htm\n",
      "  Downloading: aapl-20250927.htm\n",
      "  Downloading: aapl-20250927.xsd\n",
      "  Downloading: aapl-20250927_cal.xml\n",
      "  Downloading: aapl-20250927_def.xml\n",
      "  Downloading: aapl-20250927_htm.xml\n",
      "  Downloading: aapl-20250927_lab.xml\n",
      "  Downloading: aapl-20250927_pre.xml\n",
      "  Downloading: FilingSummary.xml\n",
      "  Downloading: R1.htm\n",
      "  Downloading: R10.htm\n",
      "  Downloading: R11.htm\n",
      "  Downloading: R12.htm\n",
      "  Downloading: R13.htm\n",
      "  Downloading: R14.htm\n",
      "  Downloading: R15.htm\n",
      "  Downloading: R16.htm\n",
      "  Downloading: R17.htm\n",
      "  Downloading: R18.htm\n",
      "  Downloading: R19.htm\n",
      "  Downloading: R2.htm\n",
      "  Downloading: R20.htm\n",
      "  Downloading: R21.htm\n",
      "  Downloading: R22.htm\n",
      "  Downloading: R23.htm\n",
      "  Downloading: R24.htm\n",
      "  Downloading: R25.htm\n",
      "  Downloading: R26.htm\n",
      "  Downloading: R27.htm\n",
      "  Downloading: R28.htm\n",
      "  Downloading: R29.htm\n",
      "  Downloading: R3.htm\n",
      "  Downloading: R30.htm\n",
      "  Downloading: R31.htm\n",
      "  Downloading: R32.htm\n",
      "  Downloading: R33.htm\n",
      "  Downloading: R34.htm\n",
      "  Downloading: R35.htm\n",
      "  Downloading: R36.htm\n",
      "  Downloading: R37.htm\n",
      "  Downloading: R38.htm\n",
      "  Downloading: R39.htm\n",
      "  Downloading: R4.htm\n",
      "  Downloading: R40.htm\n",
      "  Downloading: R41.htm\n",
      "  Downloading: R42.htm\n",
      "  Downloading: R43.htm\n",
      "  Downloading: R44.htm\n",
      "  Downloading: R45.htm\n",
      "  Downloading: R46.htm\n",
      "  Downloading: R47.htm\n",
      "  Downloading: R48.htm\n",
      "  Downloading: R49.htm\n",
      "  Downloading: R5.htm\n",
      "  Downloading: R50.htm\n",
      "  Downloading: R51.htm\n",
      "  Downloading: R52.htm\n",
      "  Downloading: R53.htm\n",
      "  Downloading: R54.htm\n",
      "  Downloading: R55.htm\n",
      "  Downloading: R56.htm\n",
      "  Downloading: R57.htm\n",
      "  Downloading: R58.htm\n",
      "  Downloading: R59.htm\n",
      "  Downloading: R6.htm\n",
      "  Downloading: R60.htm\n",
      "  Downloading: R61.htm\n",
      "  Downloading: R62.htm\n",
      "  Downloading: R63.htm\n",
      "  Downloading: R64.htm\n",
      "  Downloading: R65.htm\n",
      "  Downloading: R66.htm\n",
      "  Downloading: R67.htm\n",
      "  Downloading: R68.htm\n",
      "  Downloading: R69.htm\n",
      "  Downloading: R7.htm\n",
      "  Downloading: R70.htm\n",
      "  Downloading: R8.htm\n",
      "  Downloading: R9.htm\n",
      "  Downloading: sec_access.htm\n",
      "  Downloading: oacq.htm\n",
      "  Downloading: privacy.htm\n",
      "Downloaded 99 files to filings/0000320193_0000320193-25-000079\n",
      "\n",
      "================================================================================\n",
      "Processing: MSFT\n",
      "================================================================================\n",
      "CIK: 0000789019\n",
      "Filing Date: 2025-07-30\n",
      "Downloading from: https://www.sec.gov/Archives/edgar/data/0000789019/000095017025100235/\n",
      "  Downloading: index.htm\n",
      "  Downloading: search.htm\n",
      "  Downloading: howinvestigationswork.html\n",
      "  Downloading: brokers.htm\n",
      "  Downloading: quickedgar.htm\n",
      "  Downloading: companysearch.html\n",
      "  Downloading: secforms.htm\n",
      "  Downloading: publicdocs.htm\n",
      "  Downloading: index.html\n",
      "  Downloading: upcoming-events.htm\n",
      "  Downloading: 0000950170-25-100235-index-headers.html\n",
      "  Downloading: 0000950170-25-100235-index.html\n",
      "  Downloading: FilingSummary.xml\n",
      "  Downloading: msft-20250630.htm\n",
      "  Downloading: msft-20250630.xsd\n",
      "  Downloading: msft-20250630_htm.xml\n",
      "  Downloading: msft-ex10_7.htm\n",
      "  Downloading: msft-ex10_8.htm\n",
      "  Downloading: msft-ex21.htm\n",
      "  Downloading: msft-ex23_1.htm\n",
      "  Downloading: msft-ex31_1.htm\n",
      "  Downloading: msft-ex31_2.htm\n",
      "  Downloading: msft-ex32_1.htm\n",
      "  Downloading: msft-ex32_2.htm\n",
      "  Downloading: R1.htm\n",
      "  Downloading: R10.htm\n",
      "  Downloading: R100.htm\n",
      "  Downloading: R101.htm\n",
      "  Downloading: R102.htm\n",
      "  Downloading: R103.htm\n",
      "  Downloading: R104.htm\n",
      "  Downloading: R105.htm\n",
      "  Downloading: R106.htm\n",
      "  Downloading: R107.htm\n",
      "  Downloading: R108.htm\n",
      "  Downloading: R109.htm\n",
      "  Downloading: R11.htm\n",
      "  Downloading: R110.htm\n",
      "  Downloading: R12.htm\n",
      "  Downloading: R13.htm\n",
      "  Downloading: R14.htm\n",
      "  Downloading: R15.htm\n",
      "  Downloading: R16.htm\n",
      "  Downloading: R17.htm\n",
      "  Downloading: R18.htm\n",
      "  Downloading: R19.htm\n",
      "  Downloading: R2.htm\n",
      "  Downloading: R20.htm\n",
      "  Downloading: R21.htm\n",
      "  Downloading: R22.htm\n",
      "  Downloading: R23.htm\n",
      "  Downloading: R24.htm\n",
      "  Downloading: R25.htm\n",
      "  Downloading: R26.htm\n",
      "  Downloading: R27.htm\n",
      "  Downloading: R28.htm\n",
      "  Downloading: R29.htm\n",
      "  Downloading: R3.htm\n",
      "  Downloading: R30.htm\n",
      "  Downloading: R31.htm\n",
      "  Downloading: R32.htm\n",
      "  Downloading: R33.htm\n",
      "  Downloading: R34.htm\n",
      "  Downloading: R35.htm\n",
      "  Downloading: R36.htm\n",
      "  Downloading: R37.htm\n",
      "  Downloading: R38.htm\n",
      "  Downloading: R39.htm\n",
      "  Downloading: R4.htm\n",
      "  Downloading: R40.htm\n",
      "  Downloading: R41.htm\n",
      "  Downloading: R42.htm\n",
      "  Downloading: R43.htm\n",
      "  Downloading: R44.htm\n",
      "  Downloading: R45.htm\n",
      "  Downloading: R46.htm\n",
      "  Downloading: R47.htm\n",
      "  Downloading: R48.htm\n",
      "  Downloading: R49.htm\n",
      "  Downloading: R5.htm\n",
      "  Downloading: R50.htm\n",
      "  Downloading: R51.htm\n",
      "  Downloading: R52.htm\n",
      "  Downloading: R53.htm\n",
      "  Downloading: R54.htm\n",
      "  Downloading: R55.htm\n",
      "  Downloading: R56.htm\n",
      "  Downloading: R57.htm\n",
      "  Downloading: R58.htm\n",
      "  Downloading: R59.htm\n",
      "  Downloading: R6.htm\n",
      "  Downloading: R60.htm\n",
      "  Downloading: R61.htm\n",
      "  Downloading: R62.htm\n",
      "  Downloading: R63.htm\n",
      "  Downloading: R64.htm\n",
      "  Downloading: R65.htm\n",
      "  Downloading: R66.htm\n",
      "  Downloading: R67.htm\n",
      "  Downloading: R68.htm\n",
      "  Downloading: R69.htm\n",
      "  Downloading: R7.htm\n",
      "  Downloading: R70.htm\n",
      "  Downloading: R71.htm\n",
      "  Downloading: R72.htm\n",
      "  Downloading: R73.htm\n",
      "  Downloading: R74.htm\n",
      "  Downloading: R75.htm\n",
      "  Downloading: R76.htm\n",
      "  Downloading: R77.htm\n",
      "  Downloading: R78.htm\n",
      "  Downloading: R79.htm\n",
      "  Downloading: R8.htm\n",
      "  Downloading: R80.htm\n",
      "  Downloading: R81.htm\n",
      "  Downloading: R82.htm\n",
      "  Downloading: R83.htm\n",
      "  Downloading: R84.htm\n",
      "  Downloading: R85.htm\n",
      "  Downloading: R86.htm\n",
      "  Downloading: R87.htm\n",
      "  Downloading: R88.htm\n",
      "  Downloading: R89.htm\n",
      "  Downloading: R9.htm\n",
      "  Downloading: R90.htm\n",
      "  Downloading: R91.htm\n",
      "  Downloading: R92.htm\n",
      "  Downloading: R93.htm\n",
      "  Downloading: R94.htm\n",
      "  Downloading: R95.htm\n",
      "  Downloading: R96.htm\n",
      "  Downloading: R97.htm\n",
      "  Downloading: R98.htm\n",
      "  Downloading: R99.htm\n",
      "  Downloading: sec_access.htm\n",
      "  Downloading: oacq.htm\n",
      "  Downloading: privacy.htm\n",
      "Downloaded 137 files to filings/0000789019_0000950170-25-100235\n",
      "\n",
      "================================================================================\n",
      "Processing: GOOGL\n",
      "================================================================================\n",
      "CIK: 0001652044\n",
      "Filing Date: 2025-02-05\n",
      "Downloading from: https://www.sec.gov/Archives/edgar/data/0001652044/000165204425000014/\n",
      "  Downloading: index.htm\n",
      "  Downloading: search.htm\n",
      "  Downloading: howinvestigationswork.html\n",
      "  Downloading: brokers.htm\n",
      "  Downloading: quickedgar.htm\n",
      "  Downloading: companysearch.html\n",
      "  Downloading: secforms.htm\n",
      "  Downloading: publicdocs.htm\n",
      "  Downloading: index.html\n",
      "  Downloading: upcoming-events.htm\n",
      "  Downloading: 0001652044-25-000014-index-headers.html\n",
      "  Downloading: 0001652044-25-000014-index.html\n",
      "  Downloading: FilingSummary.xml\n",
      "  Downloading: goog-20241231.htm\n",
      "  Downloading: goog-20241231.xsd\n",
      "  Downloading: goog-20241231_cal.xml\n",
      "  Downloading: goog-20241231_def.xml\n",
      "  Downloading: goog-20241231_htm.xml\n",
      "  Downloading: goog-20241231_lab.xml\n",
      "  Downloading: goog-20241231_pre.xml\n",
      "  Downloading: googexhibit1901q42024.htm\n",
      "  Downloading: googexhibit2101q42024.htm\n",
      "  Downloading: googexhibit2301q42024.htm\n",
      "  Downloading: googexhibit3101q42024.htm\n",
      "  Downloading: googexhibit3102q42024.htm\n",
      "  Downloading: googexhibit3201q42024.htm\n",
      "  Downloading: R1.htm\n",
      "  Downloading: R10.htm\n",
      "  Downloading: R11.htm\n",
      "  Downloading: R12.htm\n",
      "  Downloading: R13.htm\n",
      "  Downloading: R14.htm\n",
      "  Downloading: R15.htm\n",
      "  Downloading: R16.htm\n",
      "  Downloading: R17.htm\n",
      "  Downloading: R18.htm\n",
      "  Downloading: R19.htm\n",
      "  Downloading: R2.htm\n",
      "  Downloading: R20.htm\n",
      "  Downloading: R21.htm\n",
      "  Downloading: R22.htm\n",
      "  Downloading: R23.htm\n",
      "  Downloading: R24.htm\n",
      "  Downloading: R25.htm\n",
      "  Downloading: R26.htm\n",
      "  Downloading: R27.htm\n",
      "  Downloading: R28.htm\n",
      "  Downloading: R29.htm\n",
      "  Downloading: R3.htm\n",
      "  Downloading: R30.htm\n",
      "  Downloading: R31.htm\n",
      "  Downloading: R32.htm\n",
      "  Downloading: R33.htm\n",
      "  Downloading: R34.htm\n",
      "  Downloading: R35.htm\n",
      "  Downloading: R36.htm\n",
      "  Downloading: R37.htm\n",
      "  Downloading: R38.htm\n",
      "  Downloading: R39.htm\n",
      "  Downloading: R4.htm\n",
      "  Downloading: R40.htm\n",
      "  Downloading: R41.htm\n",
      "  Downloading: R42.htm\n",
      "  Downloading: R43.htm\n",
      "  Downloading: R44.htm\n",
      "  Downloading: R45.htm\n",
      "  Downloading: R46.htm\n",
      "  Downloading: R47.htm\n",
      "  Downloading: R48.htm\n",
      "  Downloading: R49.htm\n",
      "  Downloading: R5.htm\n",
      "  Downloading: R50.htm\n",
      "  Downloading: R51.htm\n",
      "  Downloading: R52.htm\n",
      "  Downloading: R53.htm\n",
      "  Downloading: R54.htm\n",
      "  Downloading: R55.htm\n",
      "  Downloading: R56.htm\n",
      "  Downloading: R57.htm\n",
      "  Downloading: R58.htm\n",
      "  Downloading: R59.htm\n",
      "  Downloading: R6.htm\n",
      "  Downloading: R60.htm\n",
      "  Downloading: R61.htm\n",
      "  Downloading: R62.htm\n",
      "  Downloading: R63.htm\n",
      "  Downloading: R64.htm\n",
      "  Downloading: R65.htm\n",
      "  Downloading: R66.htm\n",
      "  Downloading: R67.htm\n",
      "  Downloading: R68.htm\n",
      "  Downloading: R69.htm\n",
      "  Downloading: R7.htm\n",
      "  Downloading: R70.htm\n",
      "  Downloading: R71.htm\n",
      "  Downloading: R72.htm\n",
      "  Downloading: R73.htm\n",
      "  Downloading: R74.htm\n",
      "  Downloading: R75.htm\n",
      "  Downloading: R76.htm\n",
      "  Downloading: R77.htm\n",
      "  Downloading: R78.htm\n",
      "  Downloading: R79.htm\n",
      "  Downloading: R8.htm\n",
      "  Downloading: R80.htm\n",
      "  Downloading: R81.htm\n",
      "  Downloading: R82.htm\n",
      "  Downloading: R83.htm\n",
      "  Downloading: R84.htm\n",
      "  Downloading: R85.htm\n",
      "  Downloading: R86.htm\n",
      "  Downloading: R87.htm\n",
      "  Downloading: R88.htm\n",
      "  Downloading: R89.htm\n",
      "  Downloading: R9.htm\n",
      "  Downloading: R90.htm\n",
      "  Downloading: R91.htm\n",
      "  Downloading: R92.htm\n",
      "  Downloading: R93.htm\n",
      "  Downloading: sec_access.htm\n",
      "  Downloading: oacq.htm\n",
      "  Downloading: privacy.htm\n",
      "Downloaded 122 files to filings/0001652044_0001652044-25-000014\n",
      "\n",
      "================================================================================\n",
      "BATCH PROCESSING SUMMARY\n",
      "================================================================================\n",
      "  ticker   status  sections_found filing_date\n",
      "0   AAPL  success               0  2025-10-31\n",
      "1   MSFT  success               0  2025-07-30\n",
      "2  GOOGL  success               0  2025-02-05\n"
     ]
    }
   ],
   "source": [
    "def process_company(ticker: str, output_base_dir: str = './filings') -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single company's 10-K filing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing: {ticker}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Get CIK\n",
    "        cik = get_company_cik(ticker)\n",
    "        print(f\"CIK: {cik}\")\n",
    "        \n",
    "        # Get latest filing\n",
    "        filing_info = get_latest_10k_filing(cik)\n",
    "        print(f\"Filing Date: {filing_info['filingDate']}\")\n",
    "        \n",
    "        # Download files\n",
    "        filing_dir = download_filing_files(cik, filing_info['accessionNumber'], output_base_dir)\n",
    "        \n",
    "        # Parse XBRL\n",
    "        xbrl_file = find_xbrl_instance_file(filing_dir)\n",
    "        if xbrl_file:\n",
    "            text_blocks = parse_xbrl_for_text_blocks(xbrl_file)\n",
    "        else:\n",
    "            text_blocks = parse_htm_for_sections(filing_dir)\n",
    "        \n",
    "        # Save results\n",
    "        output_dir = Path(filing_dir) / \"extracted_sections\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        json_file = output_dir / \"all_sections.json\"\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'ticker': ticker,\n",
    "                'cik': cik,\n",
    "                'filing_date': filing_info['filingDate'],\n",
    "                'accession_number': filing_info['accessionNumber'],\n",
    "                'sections': text_blocks\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'status': 'success',\n",
    "            'sections_found': len(text_blocks),\n",
    "            'filing_date': filing_info['filingDate']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: Process multiple companies\n",
    "tickers_to_process = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "results = []\n",
    "for ticker in tickers_to_process:\n",
    "    result = process_company(ticker)\n",
    "    results.append(result)\n",
    "\n",
    "# Display results summary\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH PROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289bfc657cf8466d8872d5bd63a2dde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6 embeddings.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a small embedding model (fast and works offline)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert your extracted text sections into embeddings\n",
    "embeddings = model.encode(\n",
    "    [chunk['text'] for chunk in all_chunks], \n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04852507 -0.00215493 -0.0519914  -0.02349418  0.09521586  0.0723874\n",
      "  0.13519742  0.04354686 -0.06532799  0.01718577]\n"
     ]
    }
   ],
   "source": [
    "# show the first 10 numbers of the first embedding (out of 384 total\n",
    "\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 384)\n"
     ]
    }
   ],
   "source": [
    "# checking structure\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb --quiet\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"sec_filings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 6 chunks\n"
     ]
    }
   ],
   "source": [
    "# Clear and add to avoid duplicates if rerunning\n",
    "try:\n",
    "    client.delete_collection(\"sec_filings\")\n",
    "    collection = client.get_or_create_collection(\"sec_filings\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ids = [f\"chunk_{i}\" for i in range(len(all_chunks))]\n",
    "docs = [c[\"text\"] for c in all_chunks]\n",
    "\n",
    "collection.add(ids=ids, embeddings=embeddings.tolist(), documents=docs)\n",
    "print(f\"Stored {len(ids)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1 (score=1.1942):\n",
      "cerning supply chain constraints, our strategy, competition, future operations and production capacity, future financial position, future revenues, projected costs, profitability, expected cost reductions, capital adequacy, expectations regarding demand and acceptance for our technologies, growth opportunities and trends in the markets in which we operate, prospects and plans and objectives of man...\n",
      "\n",
      "Result 2 (score=1.3536):\n",
      "m 12.Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters93Item 13.Certain Relationships and Related Transactions, and Director Independence93Item 14.Principal Accountant Fees and Services93 PART IV. Item 15.Exhibits and Financial Statement Schedules94Item 16.Form 10-K Summary107 SignaturesTable of ContentsForward-Looking StatementsThe discussions in this ...\n",
      "\n",
      "Result 3 (score=1.4526):\n",
      "Item 1C.Cybersecurity28Item 2.Properties29Item 3.Legal Proceedings29Item 4.Mine Safety Disclosures29 PART II. Item 5.Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities30Item 6.[Reserved]31Item 7.Management's Discussion and Analysis of Financial Condition and Results of Operations32Item 7A.Quantitative and Qualitative Disclosures about Marke...\n"
     ]
    }
   ],
   "source": [
    "res = collection.query(query_texts=[\"supply chain risks\"], n_results=3)\n",
    "for i, (doc, score) in enumerate(zip(res[\"documents\"][0], res[\"distances\"][0])):\n",
    "    print(f\"\\nResult {i+1} (score={score:.4f}):\\n{doc[:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your_api_key_here\"  # replace with your real key\n",
    "client = OpenAI()\n",
    "\n",
    "def ask_gpt_rag(query, k=3, model=\"gpt-4o-mini\"):\n",
    "    # Retrieve context from Chroma\n",
    "    res = collection.query(query_texts=[query], n_results=k)\n",
    "    context = \"\\n\\n\".join(res[\"documents\"][0])\n",
    "\n",
    "    # Build the RAG prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a financial analyst.\n",
    "    Use the context below to answer the question accurately and concisely.\n",
    "    If the answer isn’t clearly mentioned, say “Not mentioned in the filings.”\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Send to GPT\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import RateLimitError\n",
    "\n",
    "def ask_gpt_rag(query, context, model=\"gpt-4-turbo\"):\n",
    "    client = OpenAI()\n",
    "    prompt = f\"\"\"\n",
    "    You are a financial analyst.\n",
    "    Use the context below to answer the question accurately and concisely.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retry logic for rate limit errors\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            return completion.choices[0].message.content.strip()\n",
    "        \n",
    "        except RateLimitError:\n",
    "            wait = 15 * (attempt + 1)\n",
    "            print(f\"⚠️ Rate limit reached — waiting {wait} seconds before retry...\")\n",
    "            time.sleep(wait)\n",
    "    \n",
    "    raise Exception(\"Failed after multiple retry attempts due to rate limits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = collection.query(query_texts=[\"supply chain risks\"], n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \" \".join(res[\"documents\"][0])  # combine top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, APIError, RateLimitError, AuthenticationError, NotFoundError\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-4o\"        # or \"gpt-4o-mini\" if you keep hitting rate limits\n",
    "\n",
    "def ask_gpt_rag(query: str, context: str, model: str = MODEL,\n",
    "                temperature: float = 0.2, max_retries: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Send a RAG-style prompt (question + retrieved context) to an OpenAI chat model\n",
    "    with exponential backoff on transient errors.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial analyst. Answer concisely using ONLY the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return completion.choices[0].message.content.strip()\n",
    "\n",
    "        except (RateLimitError, APIError) as e:\n",
    "            # transient -> backoff and retry\n",
    "            last_err = e\n",
    "            time.sleep(2 ** attempt)\n",
    "            continue\n",
    "\n",
    "        except (AuthenticationError, NotFoundError):\n",
    "            # permanent -> surface immediately (bad key/model name, etc.)\n",
    "            raise\n",
    "\n",
    "    raise RuntimeError(f\"Failed after {max_retries} attempts. Last error: {last_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, time\n",
    "# from datetime import datetime\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Ensure your API key is loaded\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# # ✅ Example: list of S&P500 companies (you can expand this list to all 500)\n",
    "# companies = [\n",
    "#     \"MMM\",\"AOS\",\"ABT\",\"ABBV\",\"ACN\",\"ADBE\",\"AAP\",\"AES\",\"AFL\",\"A\",\"APD\",\"AKAM\",\"ALK\",\"ALB\",\n",
    "#     \"ARE\",\"ALGN\",\"ALLE\",\"LNT\",\"ALL\",\"GOOGL\",\"MO\",\"AMZN\",\"AMCR\",\"AAL\",\"AEP\",\"AXP\",\"AIG\",\n",
    "#     \"AMT\",\"AWK\",\"AMP\",\"COR\",\"AME\",\"AMGN\",\"APH\",\"ADI\",\"ANSS\",\"AON\",\"APA\",\"AAPL\",\"AMAT\",\n",
    "#     \"APTV\",\"ADM\",\"ANET\",\"AJG\",\"AIZ\",\"T\",\"ATO\",\"ADSK\",\"AZO\",\"AVB\",\"AVY\",\"BKR\",\"BALL\",\n",
    "#     \"BAC\",\"BBWI\",\"BAX\",\"BDX\",\"BRK.B\",\"BBY\",\"BIO\",\"TECH\",\"BIIB\",\"BLK\",\"BA\",\"BKNG\",\"BWA\",\n",
    "#     \"BXP\",\"BSX\",\"BMY\",\"AVGO\",\"BR\",\"BRO\",\"BF.B\",\"CHRW\",\"CDNS\",\"CZR\",\"CPT\",\"CPB\",\"COF\",\n",
    "#     \"CAH\",\"KMX\",\"CCL\",\"CARR\",\"CTLT\",\"CAT\",\"CBOE\",\"CBRE\",\"CDW\",\"CE\",\"CNC\",\"CNP\",\"CDAY\",\n",
    "#     \"CF\",\"CRL\",\"SCHW\",\"CHTR\",\"CVX\",\"CMG\",\"CB\",\"CHD\",\"CI\",\"CINF\",\"CTAS\",\"CSCO\",\"C\",\"CME\",\n",
    "#     \"CMS\",\"KO\",\"CTSH\",\"CL\",\"CMCSA\",\"CMA\",\"CAG\",\"COP\",\"ED\",\"STZ\",\"CEG\",\"CPRT\",\"GLW\",\"CTVA\",\n",
    "#     \"COST\",\"CTRA\",\"CCI\",\"CSX\",\"CMI\",\"CVS\",\"DHI\",\"DHR\",\"DRI\",\"DVA\",\"DE\",\"DAL\",\"XRAY\",\"DVN\",\n",
    "#     \"DXCM\",\"FANG\",\"DLR\",\"DFS\",\"DISH\",\"DIS\",\"DG\",\"DLTR\",\"D\",\"DOV\",\"DOW\",\"DTE\",\"DUK\",\"DD\",\n",
    "#     \"DXC\",\"EMN\",\"ETN\",\"EBAY\",\"ECL\",\"EIX\",\"EW\",\"EA\",\"EMR\",\"ENPH\",\"ETR\",\"EOG\",\"EPAM\",\"EFX\",\n",
    "#     \"EQIX\",\"EQR\",\"ESS\",\"EL\",\"ETSY\",\"EG\",\"EVRG\",\"ES\",\"EXC\",\"EXPE\",\"EXPD\",\"EXR\",\"XOM\",\"FFIV\",\n",
    "#     \"FAST\",\"FRT\",\"FDX\",\"FIS\",\"FITB\",\"FSLR\",\"FE\",\"FI\",\"FLT\",\"FMC\",\"F\",\"FTNT\",\"FTV\",\"FOX\",\n",
    "#     \"BEN\",\"FCX\",\"GRMN\",\"IT\",\"GE\",\"GD\",\"GIS\",\"GM\",\"GILD\",\"GPN\",\"GL\",\"GS\",\"HAL\",\"HIG\",\"HAS\",\n",
    "#     \"HCA\",\"DOC\",\"HSIC\",\"HSY\",\"HES\",\"HPE\",\"HLT\",\"HOLX\",\"HD\",\"HON\",\"HRL\",\"HST\",\"HWM\",\"HPQ\",\n",
    "#     \"HUM\",\"HBAN\",\"HII\",\"IBM\",\"IEX\",\"IDXX\",\"ITW\",\"ILMN\",\"INCY\",\"IR\",\"INTC\",\"ICE\",\"IP\",\"IPG\",\n",
    "#     \"IFF\",\"INTU\",\"ISRG\",\"IVZ\",\"IRM\",\"JBHT\",\"JKHY\",\"J\",\"JNJ\",\"JCI\",\"JPM\",\"JNPR\",\"K\",\"KDP\",\n",
    "#     \"KEY\",\"KMB\",\"KIM\",\"KMI\",\"KLAC\",\"KR\",\"LHX\",\"LH\",\"LRCX\",\"LW\",\"LVS\",\"LDOS\",\"LEN\",\"LNC\",\n",
    "#     \"LIN\",\"LYV\",\"LKQ\",\"LMT\",\"L\",\"LOW\",\"LUMN\",\"LYB\",\"MTB\",\"MRO\",\"MPC\",\"MKTX\",\"MAR\",\"MMC\",\n",
    "#     \"MLM\",\"MAS\",\"MA\",\"MKC\",\"MCD\",\"MCK\",\"MDT\",\"MRK\",\"META\",\"MET\",\"MTD\",\"MCHP\",\"MU\",\"MSFT\",\n",
    "#     \"MAA\",\"MRNA\",\"MDLZ\",\"MNST\",\"MCO\",\"MS\",\"MSI\",\"MSCI\",\"NDAQ\",\"NTAP\",\"NFLX\",\"NWL\",\"NEM\",\n",
    "#     \"NWSA\",\"NEE\",\"NKE\",\"NI\",\"NDSN\",\"NSC\",\"NTRS\",\"NOC\",\"NUE\",\"NVDA\",\"NXPI\",\"OXY\",\"ODFL\",\n",
    "#     \"OMC\",\"OKE\",\"ORCL\",\"OTIS\",\"PCAR\",\"PKG\",\"PARA\",\"PH\",\"PAYX\",\"PYPL\",\"PEP\",\"PFE\",\"PM\",\n",
    "#     \"PSX\",\"PNC\",\"PPG\",\"PG\",\"PGR\",\"PLD\",\"PRU\",\"PEG\",\"PHM\",\"QCOM\",\"PWR\",\"DGX\",\"RJF\",\"RTX\",\n",
    "#     \"RF\",\"RSG\",\"RMD\",\"RHI\",\"ROK\",\"ROL\",\"ROP\",\"ROST\",\"SPGI\",\"CRM\",\"NOW\",\"SHW\",\"SPG\",\"SWKS\",\n",
    "#     \"SNA\",\"SO\",\"LUV\",\"SWK\",\"STT\",\"SYK\",\"TMUS\",\"TPR\",\"TGT\",\"TEL\",\"TXT\",\"CLX\",\"HSY\",\"TRV\",\n",
    "#     \"TMO\",\"TJX\",\"TSCO\",\"TT\",\"TDG\",\"TROW\",\"TFC\",\"TSN\",\"UNP\",\"UAL\",\"UPS\",\"URI\",\"UNH\",\"VLO\",\n",
    "#     \"VZ\",\"VRTX\",\"V\",\"VMC\",\"WMT\",\"DIS\",\"WBD\",\"WM\",\"WFC\",\"WELL\",\"WST\",\"WDC\",\"WY\",\"WHR\",\"WMB\",\n",
    "#     \"XEL\",\"XYL\",\"YUM\",\"ZBH\",\"ZION\",\"ZTS\"\n",
    "# ]\n",
    "\n",
    "# # Base question template\n",
    "# question_template = \"What supply chain risks did {company} mention in its 10-K report?\"\n",
    "\n",
    "# # Directory to store outputs (optional)\n",
    "# output_dir = \"sp500_json_outputs\"\n",
    "\n",
    "# import os\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# for company in companies:\n",
    "#     query = question_template.format(company=company)\n",
    "#     context = f\"Analyze the 10-K filing of {company} and summarize its supply chain risks.\"\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# You are a financial analyst. Answer professionally and concisely.\n",
    "# Question: {query}\n",
    "# Context: {context}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "#     print(f\"\\n🔍 Processing {company}...\")\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4o-mini\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#             temperature=0.2,\n",
    "#         )\n",
    "#         answer = response.choices[0].message.content.strip()\n",
    "\n",
    "#         # Save to JSON\n",
    "#         timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#         output = {\n",
    "#             \"company\": company,\n",
    "#             \"query\": query,\n",
    "#             \"response\": answer,\n",
    "#             \"timestamp\": timestamp\n",
    "#         }\n",
    "\n",
    "#         filename = os.path.join(output_dir, f\"{company.replace(' ', '_')}_{timestamp}.json\")\n",
    "#         with open(filename, \"w\") as f:\n",
    "#             json.dump(output, f, indent=4)\n",
    "\n",
    "#         print(f\"✅ Saved {company} → {filename}\")\n",
    "\n",
    "#         # Prevent API overload\n",
    "#         time.sleep(2)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error with {company}: {e}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 503 companies from StockAnalysis list.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://stockanalysis.com/list/sp-500-stocks/\"\n",
    "df  = pd.read_html(url)[0]\n",
    "companies = df[\"Company Name\"].tolist()   # or whichever column holds the company name\n",
    "print(f\"✅ Loaded {len(companies)} companies from StockAnalysis list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 503 tickers from StockAnalysis S&P 500 list.\n",
      "['NVDA', 'AAPL', 'GOOGL', 'GOOG', 'MSFT', 'AMZN', 'AVGO', 'META', 'TSLA', 'BRK.B', 'LLY', 'WMT', 'JPM', 'V', 'ORCL', 'XOM', 'JNJ', 'MA', 'NFLX', 'ABBV', 'COST', 'BAC', 'PLTR', 'PG', 'HD', 'AMD', 'KO', 'GE', 'CVX', 'CSCO', 'UNH', 'IBM', 'WFC', 'CAT', 'MS', 'AXP', 'GS', 'MRK', 'PM', 'TMUS', 'MU', 'RTX', 'ABT', 'TMO', 'MCD', 'CRM', 'PEP', 'ISRG', 'LIN', 'DIS', 'INTU', 'T', 'AMGN', 'LRCX', 'AMAT', 'C', 'APP', 'BX', 'QCOM', 'UBER', 'NEE', 'VZ', 'NOW', 'TJX', 'BLK', 'INTC', 'APH', 'SCHW', 'DHR', 'GILD', 'ACN', 'BKNG', 'GEV', 'SPGI', 'ANET', 'TXN', 'KLAC', 'BSX', 'PFE', 'SYK', 'WELL', 'BA', 'ADBE', 'UNP', 'PGR', 'COF', 'DE', 'LOW', 'MDT', 'ETN', 'PANW', 'CRWD', 'HON', 'PLD', 'CB', 'ADI', 'HCA', 'VRTX', 'COP', 'MCK', 'LMT', 'PH', 'KKR', 'CEG', 'ADP', 'CMCSA', 'CVS', 'CME', 'SO', 'MO', 'SBUX', 'HOOD', 'DUK', 'BMY', 'NKE', 'GD', 'NEM', 'TT', 'MMM', 'MMC', 'ICE', 'WM', 'MCO', 'ORLY', 'AMT', 'SHW', 'DELL', 'CDNS', 'DASH', 'NOC', 'UPS', 'MAR', 'HWM', 'REGN', 'TDG', 'ECL', 'APO', 'CTAS', 'AON', 'CI', 'USB', 'BK', 'EQIX', 'MDLZ', 'PNC', 'WMB', 'SNPS', 'EMR', 'RCL', 'ITW', 'ELV', 'COR', 'MNST', 'JCI', 'ABNB', 'SPG', 'GLW', 'RSG', 'GM', 'CL', 'CMI', 'AZO', 'COIN', 'TRV', 'AJG', 'AEP', 'TEL', 'NSC', 'PWR', 'CSX', 'HLT', 'FDX', 'ADSK', 'MSI', 'SRE', 'WDAY', 'KMI', 'FTNT', 'TFC', 'AFL', 'EOG', 'IDXX', 'WBD', 'MPC', 'APD', 'FCX', 'VST', 'PYPL', 'ROST', 'ALL', 'DDOG', 'BDX', 'DLR', 'PCAR', 'SLB', 'PSX', 'ZTS', 'VLO', 'D', 'O', 'LHX', 'STX', 'F', 'URI', 'NDAQ', 'EA', 'CAH', 'MET', 'EW', 'BKR', 'NXPI', 'ROP', 'WDC', 'PSA', 'XEL', 'EXC', 'CBRE', 'FAST', 'GWW', 'AME', 'OKE', 'CTVA', 'CARR', 'KR', 'TTWO', 'LVS', 'A', 'DHI', 'ROK', 'YUM', 'FICO', 'ETR', 'MSCI', 'FANG', 'CMG', 'MPWR', 'AMP', 'AXON', 'AIG', 'OXY', 'PEG', 'PAYX', 'TGT', 'CPRT', 'CCI', 'IQV', 'VMC', 'HIG', 'DAL', 'HSY', 'KDP', 'XYZ', 'VTR', 'PRU', 'GRMN', 'SYY', 'CTSH', 'TRGP', 'RMD', 'EBAY', 'MLM', 'WEC', 'ED', 'EQT', 'KMB', 'CCL', 'NUE', 'GEHC', 'TKO', 'PCG', 'OTIS', 'WAB', 'XYL', 'ACGL', 'FIS', 'FISV', 'EL', 'STT', 'KVUE', 'LEN', 'VRSK', 'IR', 'VICI', 'NRG', 'LYV', 'EXPE', 'RJF', 'WTW', 'KHC', 'UAL', 'KEYS', 'WRB', 'MTD', 'CHTR', 'FOXA', 'EXR', 'K', 'ROL', 'MTB', 'CSGP', 'ATO', 'AEE', 'DTE', 'ADM', 'ODFL', 'FITB', 'TSCO', 'FOX', 'MCHP', 'BRO', 'EXE', 'HUM', 'IBKR', 'FE', 'HPE', 'SYF', 'FSLR', 'PPL', 'BR', 'CBOE', 'EFX', 'EME', 'CINF', 'AWK', 'STE', 'CNP', 'GIS', 'BIIB', 'AVB', 'DOV', 'IRM', 'HBAN', 'TER', 'VLTO', 'ES', 'NTRS', 'LDOS', 'EQR', 'DXCM', 'WAT', 'PHM', 'VRSN', 'PODD', 'STZ', 'TDY', 'ULTA', 'STLD', 'EIX', 'CMS', 'CFG', 'HUBB', 'HPQ', 'DG', 'DVN', 'PPG', 'LH', 'L', 'TROW', 'RF', 'HAL', 'TPR', 'WSM', 'NTAP', 'DGX', 'JBL', 'NVR', 'SBAC', 'RL', 'TPL', 'PTC', 'DLTR', 'NI', 'TYL', 'DRI', 'CPAY', 'CHD', 'INCY', 'LULU', 'IP', 'CTRA', 'AMCR', 'WST', 'KEY', 'SMCI', 'EXPD', 'TTD', 'TSN', 'ON', 'PFG', 'TRMB', 'MKC', 'BG', 'ZBH', 'CDW', 'CNC', 'CHRW', 'GPC', 'PKG', 'SW', 'LNT', 'SNA', 'EVRG', 'PSKY', 'ESS', 'GPN', 'INVH', 'IFF', 'GDDY', 'PNR', 'LUV', 'IT', 'FTV', 'HOLX', 'GEN', 'LII', 'DD', 'BBY', 'MAA', 'APTV', 'Q', 'JBHT', 'DOW', 'WY', 'ERIE', 'J', 'NWS', 'COO', 'UHS', 'OMC', 'LYB', 'NWSA', 'SOLV', 'TXT', 'ALLE', 'KIM', 'DPZ', 'ALB', 'FFIV', 'BF.B', 'BALL', 'REG', 'AVY', 'NDSN', 'EG', 'MAS', 'UDR', 'AKAM', 'IEX', 'CLX', 'DOC', 'HRL', 'DECK', 'BXP', 'JKHY', 'WYNN', 'CF', 'ZBRA', 'HST', 'VTRS', 'HII', 'CPT', 'AIZ', 'BEN', 'SJM', 'BLDR', 'RVTY', 'HAS', 'DAY', 'PNW', 'GL', 'FDS', 'IVZ', 'SWK', 'ALGN', 'EPAM', 'AES', 'TECH', 'CPB', 'BAX', 'IPG', 'SWKS', 'MRNA', 'TAP', 'AOS', 'POOL', 'MGM', 'PAYC', 'ARE', 'HSIC', 'GNRC', 'FRT', 'CAG', 'APA', 'DVA', 'NCLH', 'CRL', 'LW', 'MOS', 'MTCH', 'LKQ', 'MOH', 'SOLS', 'MHK']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load table directly from StockAnalysis\n",
    "url = \"https://stockanalysis.com/list/sp-500-stocks/\"\n",
    "df = pd.read_html(url)[0]\n",
    "\n",
    "# Extract only ticker symbols\n",
    "tickers = df[\"Symbol\"].dropna().tolist()\n",
    "\n",
    "print(f\"✅ Loaded {len(tickers)} tickers from StockAnalysis S&P 500 list.\")\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 503 tickers from StockAnalysis.\n",
      "Example tickers: ['NVDA', 'AAPL', 'GOOGL', 'GOOG', 'MSFT', 'AMZN', 'AVGO', 'META', 'TSLA', 'BRK.B']\n",
      "✅ Already completed: 504 | 🕒 Remaining: 0\n",
      "\n",
      "🚀 Starting parallel processing with 10 threads...\n",
      "\n",
      "\n",
      "🎉 Completed batch run! ✅ 0 tickers processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "#Connect to OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "#Fetch tickers from StockAnalysis (live list)\n",
    "url = \"https://stockanalysis.com/list/sp-500-stocks/\"\n",
    "df = pd.read_html(url)[0]\n",
    "tickers = df[\"Symbol\"].dropna().tolist()\n",
    "print(f\"✅ Loaded {len(tickers)} tickers from StockAnalysis.\")\n",
    "print(\"Example tickers:\", tickers[:10])\n",
    "\n",
    "#Output folder\n",
    "output_dir = \"/Users/brucewayne/Downloads/sp500_json_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Skip already-completed tickers\n",
    "completed = {f.split('_')[0] for f in os.listdir(output_dir) if f.endswith(\".json\")}\n",
    "remaining = [t for t in tickers if t not in completed]\n",
    "print(f\"✅ Already completed: {len(completed)} | 🕒 Remaining: {len(remaining)}\")\n",
    "\n",
    "#Template question\n",
    "question_template = \"What supply chain risks did {ticker} mention in its 10-K report?\"\n",
    "\n",
    "#Worker function for each ticker\n",
    "def process_ticker(ticker):\n",
    "    query = question_template.format(ticker=ticker)\n",
    "    context = f\"Analyze the latest 10-K filing of {ticker} and summarize its main supply chain risks.\"\n",
    "    prompt = f\"\"\"\n",
    "You are a financial analyst.\n",
    "Answer concisely and factually.\n",
    "Question: {query}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        output = {\n",
    "            \"ticker\": ticker,\n",
    "            \"query\": query,\n",
    "            \"response\": answer,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "\n",
    "        filename = os.path.join(output_dir, f\"{ticker}_{timestamp}.json\")\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(output, f, indent=4)\n",
    "\n",
    "        print(f\"✅ Saved {ticker}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {ticker}: {e}\")\n",
    "        return False\n",
    "\n",
    "#Run 10 tickers in parallel threads\n",
    "max_threads = 10\n",
    "print(f\"\\n🚀 Starting parallel processing with {max_threads} threads...\\n\")\n",
    "\n",
    "success_count = 0\n",
    "with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "    futures = {executor.submit(process_ticker, t): t for t in remaining}\n",
    "    for future in as_completed(futures):\n",
    "        ticker = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {ticker} failed: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 Completed batch run! ✅ {success_count} tickers processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, time\n",
    "# from datetime import datetime\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Ensure your API key is loaded\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# # ✅ Example: list of S&P500 companies (you can expand this list to all 500)\n",
    "# companies = [\n",
    "#     \"3M\",\n",
    "#     \"A. O. Smith\",\n",
    "#     \"Abbott Laboratories\",\n",
    "#     \"AbbVie\",\n",
    "#     \"Accenture\",\n",
    "#     \"Adobe\",\n",
    "#     \"Advance Auto Parts\",\n",
    "#     \"AES Corporation\",\n",
    "#     \"Aflac\",\n",
    "#     \"Agilent Technologies\",\n",
    "#     \"Air Products and Chemicals\",\n",
    "#     \"Akamai Technologies\",\n",
    "#     \"Alaska Air Group\",\n",
    "#     \"Albemarle Corporation\",\n",
    "#     \"Alexandria Real Estate Equities\",\n",
    "#     \"Align Technology\",\n",
    "#     \"Allegion\",\n",
    "#     \"Alliant Energy\",\n",
    "#     \"Allstate\",\n",
    "#     \"Alphabet\",\n",
    "#     \"Altria Group\",\n",
    "#     \"Amazon\",\n",
    "#     \"Amcor\",\n",
    "#     \"American Airlines Group\",\n",
    "#     \"American Electric Power\",\n",
    "#     \"American Express\",\n",
    "#     \"American International Group\",\n",
    "#     \"American Tower\",\n",
    "#     \"American Water Works\",\n",
    "#     \"Ameriprise Financial\",\n",
    "#     \"AmerisourceBergen\",\n",
    "#     \"Ametek\",\n",
    "#     \"Amgen\",\n",
    "#     \"Amphenol\",\n",
    "#     \"Analog Devices\",\n",
    "#     \"ANSYS\",\n",
    "#     \"Aon\",\n",
    "#     \"APA Corporation\",\n",
    "#     \"Apple\",\n",
    "#     \"Applied Materials\",\n",
    "#     \"Aptiv\",\n",
    "#     \"Archer-Daniels-Midland\",\n",
    "#     \"Arista Networks\",\n",
    "#     \"Arthur J. Gallagher & Co.\",\n",
    "#     \"Assurant\",\n",
    "#     \"AT&T\",\n",
    "#     \"Atmos Energy\",\n",
    "#     \"Autodesk\",\n",
    "#     \"AutoZone\",\n",
    "#     \"AvalonBay Communities\",\n",
    "#     \"Avery Dennison\",\n",
    "#     \"Baker Hughes\",\n",
    "#     \"Ball Corporation\",\n",
    "#     \"Bank of America\",\n",
    "#     \"Bath & Body Works\",\n",
    "#     \"Baxter International\",\n",
    "#     \"Becton Dickinson\",\n",
    "#     \"Berkshire Hathaway\",\n",
    "#     \"Best Buy\",\n",
    "#     \"Bio-Rad Laboratories\",\n",
    "#     \"Bio-Techne\",\n",
    "#     \"Biogen\",\n",
    "#     \"BlackRock\",\n",
    "#     \"Boeing\",\n",
    "#     \"Booking Holdings\",\n",
    "#     \"BorgWarner\",\n",
    "#     \"Boston Properties\",\n",
    "#     \"Boston Scientific\",\n",
    "#     \"Bristol Myers Squibb\",\n",
    "#     \"Broadcom\",\n",
    "#     \"Broadridge Financial Solutions\",\n",
    "#     \"Brown & Brown\",\n",
    "#     \"Brown–Forman\",\n",
    "#     \"C.H. Robinson\",\n",
    "#     \"Cadence Design Systems\",\n",
    "#     \"Caesars Entertainment\",\n",
    "#     \"Camden Property Trust\",\n",
    "#     \"Campbell Soup Company\",\n",
    "#     \"Capital One\",\n",
    "#     \"Cardinal Health\",\n",
    "#     \"CarMax\",\n",
    "#     \"Carnival\",\n",
    "#     \"Carrier Global\",\n",
    "#     \"Catalent\",\n",
    "#     \"Caterpillar\",\n",
    "#     \"Cboe Global Markets\",\n",
    "#     \"CBRE Group\",\n",
    "#     \"CDW\",\n",
    "#     \"Celanese\",\n",
    "#     \"Centene Corporation\",\n",
    "#     \"CenterPoint Energy\",\n",
    "#     \"Ceridian\",\n",
    "#     \"CF Industries\",\n",
    "#     \"Charles River Laboratories\",\n",
    "#     \"Charles Schwab Corporation\",\n",
    "#     \"Charter Communications\",\n",
    "#     \"Chevron Corporation\",\n",
    "#     \"Chipotle Mexican Grill\",\n",
    "#     \"Chubb Limited\",\n",
    "#     \"Church & Dwight\",\n",
    "#     \"Cigna\",\n",
    "#     \"Cincinnati Financial\",\n",
    "#     \"Cintas\",\n",
    "#     \"Cisco\",\n",
    "#     \"Citigroup\",\n",
    "#     \"CME Group\",\n",
    "#     \"CMS Energy\",\n",
    "#     \"Coca-Cola Company\",\n",
    "#     \"Cognizant\",\n",
    "#     \"Colgate-Palmolive\",\n",
    "#     \"Comcast\",\n",
    "#     \"Comerica\",\n",
    "#     \"Conagra Brands\",\n",
    "#     \"ConocoPhillips\",\n",
    "#     \"Consolidated Edison\",\n",
    "#     \"Constellation Brands\",\n",
    "#     \"Constellation Energy\",\n",
    "#     \"Copart\",\n",
    "#     \"Corning\",\n",
    "#     \"Corteva\",\n",
    "#     \"Costco\",\n",
    "#     \"Coterra\",\n",
    "#     \"Crown Castle\",\n",
    "#     \"CSX\",\n",
    "#     \"Cummins\",\n",
    "#     \"CVS Health\",\n",
    "#     \"D.R. Horton\",\n",
    "#     \"Danaher Corporation\",\n",
    "#     \"Darden Restaurants\",\n",
    "#     \"DaVita\",\n",
    "#     \"Deere & Company\",\n",
    "#     \"Delta Air Lines\",\n",
    "#     \"Dentsply Sirona\",\n",
    "#     \"Devon Energy\",\n",
    "#     \"Dexcom\",\n",
    "#     \"Diamondback Energy\",\n",
    "#     \"Digital Realty\",\n",
    "#     \"Discover Financial\",\n",
    "#     \"Dish Network\",\n",
    "#     \"Disney\",\n",
    "#     \"Dollar General\",\n",
    "#     \"Dollar Tree\",\n",
    "#     \"Dominion Energy\",\n",
    "#     \"Dover Corporation\",\n",
    "#     \"Dow\",\n",
    "#     \"DTE Energy\",\n",
    "#     \"Duke Energy\",\n",
    "#     \"DuPont\",\n",
    "#     \"DXC Technology\",\n",
    "#     \"Eastman Chemical Company\",\n",
    "#     \"Eaton Corporation\",\n",
    "#     \"eBay\",\n",
    "#     \"Ecolab\",\n",
    "#     \"Edison International\",\n",
    "#     \"Edwards Lifesciences\",\n",
    "#     \"Electronic Arts\",\n",
    "#     \"Emerson Electric\",\n",
    "#     \"Enphase Energy\",\n",
    "#     \"Entergy\",\n",
    "#     \"EOG Resources\",\n",
    "#     \"EPAM Systems\",\n",
    "#     \"Equifax\",\n",
    "#     \"Equinix\",\n",
    "#     \"Equity Residential\",\n",
    "#     \"Essex Property Trust\",\n",
    "#     \"Estée Lauder Companies\",\n",
    "#     \"Etsy\",\n",
    "#     \"Everest Re\",\n",
    "#     \"Evergy\",\n",
    "#     \"Eversource Energy\",\n",
    "#     \"Exelon\",\n",
    "#     \"Expedia Group\",\n",
    "#     \"Expeditors International\",\n",
    "#     \"Extra Space Storage\",\n",
    "#     \"ExxonMobil\",\n",
    "#     \"F5, Inc.\",\n",
    "#     \"Fastenal\",\n",
    "#     \"Federal Realty\",\n",
    "#     \"FedEx\",\n",
    "#     \"Fidelity National Information Services\",\n",
    "#     \"Fifth Third Bank\",\n",
    "#     \"First Solar\",\n",
    "#     \"FirstEnergy\",\n",
    "#     \"FISERV\",\n",
    "#     \"FleetCor Technologies\",\n",
    "#     \"FMC Corporation\",\n",
    "#     \"Ford\",\n",
    "#     \"Fortinet\",\n",
    "#     \"Fortive\",\n",
    "#     \"Fox Corporation\",\n",
    "#     \"Franklin Templeton\",\n",
    "#     \"Freeport-McMoRan\",\n",
    "#     \"Garmin\",\n",
    "#     \"Gartner\",\n",
    "#     \"GE Aerospace\",\n",
    "#     \"General Dynamics\",\n",
    "#     \"General Electric\",\n",
    "#     \"General Mills\",\n",
    "#     \"General Motors\",\n",
    "#     \"Gilead Sciences\",\n",
    "#     \"Global Payments\",\n",
    "#     \"Globe Life\",\n",
    "#     \"Goldman Sachs\",\n",
    "#     \"Halliburton\",\n",
    "#     \"Hartford\",\n",
    "#     \"Hasbro\",\n",
    "#     \"HCA Healthcare\",\n",
    "#     \"Healthpeak Properties\",\n",
    "#     \"Henry Schein\",\n",
    "#     \"Hershey's\",\n",
    "#     \"Hess Corporation\",\n",
    "#     \"Hewlett Packard Enterprise\",\n",
    "#     \"Hilton Worldwide\",\n",
    "#     \"Hologic\",\n",
    "#     \"Home Depot\",\n",
    "#     \"Honeywell\",\n",
    "#     \"Hormel Foods\",\n",
    "#     \"Host Hotels & Resorts\",\n",
    "#     \"Howmet Aerospace\",\n",
    "#     \"HP Inc.\",\n",
    "#     \"Humana\",\n",
    "#     \"Huntington Bancshares\",\n",
    "#     \"Huntington Ingalls Industries\",\n",
    "#     \"IBM\",\n",
    "#     \"IDEX Corporation\",\n",
    "#     \"IDEXX Laboratories\",\n",
    "#     \"Illinois Tool Works\",\n",
    "#     \"Illumina\",\n",
    "#     \"Incyte\",\n",
    "#     \"Ingersoll Rand\",\n",
    "#     \"Intel\",\n",
    "#     \"Intercontinental Exchange\",\n",
    "#     \"International Paper\",\n",
    "#     \"Interpublic Group\",\n",
    "#     \"International Flavors & Fragrances\",\n",
    "#     \"Intuit\",\n",
    "#     \"Intuitive Surgical\",\n",
    "#     \"Invesco\",\n",
    "#     \"Iron Mountain\",\n",
    "#     \"J.B. Hunt\",\n",
    "#     \"Jack Henry & Associates\",\n",
    "#     \"Jacobs Solutions\",\n",
    "#     \"Johnson & Johnson\",\n",
    "#     \"Johnson Controls\",\n",
    "#     \"JPMorgan Chase\",\n",
    "#     \"Juniper Networks\",\n",
    "#     \"Kellogg's\",\n",
    "#     \"Keurig Dr Pepper\",\n",
    "#     \"KeyCorp\",\n",
    "#     \"Kimberly-Clark\",\n",
    "#     \"Kimco Realty\",\n",
    "#     \"Kinder Morgan\",\n",
    "#     \"KLA Corporation\",\n",
    "#     \"Kroger\",\n",
    "#     \"L3Harris Technologies\",\n",
    "#     \"LabCorp\",\n",
    "#     \"Lam Research\",\n",
    "#     \"Lamb Weston\",\n",
    "#     \"Las Vegas Sands\",\n",
    "#     \"Leidos\",\n",
    "#     \"Lennar\",\n",
    "#     \"Lincoln National\",\n",
    "#     \"Linde plc\",\n",
    "#     \"Live Nation\",\n",
    "#     \"LKQ Corporation\",\n",
    "#     \"Lockheed Martin\",\n",
    "#     \"Loews Corporation\",\n",
    "#     \"Lowe's\",\n",
    "#     \"Lumen Technologies\",\n",
    "#     \"LyondellBasell\",\n",
    "#     \"M&T Bank\",\n",
    "#     \"Marathon Oil\",\n",
    "#     \"Marathon Petroleum\",\n",
    "#     \"MarketAxess\",\n",
    "#     \"Marriott International\",\n",
    "#     \"Marsh & McLennan\",\n",
    "#     \"Martin Marietta Materials\",\n",
    "#     \"Masco\",\n",
    "#     \"Mastercard\",\n",
    "#     \"McCormick & Company\",\n",
    "#     \"McDonald's\",\n",
    "#     \"McKesson\",\n",
    "#     \"Medtronic\",\n",
    "#     \"Merck & Co.\",\n",
    "#     \"Meta Platforms\",\n",
    "#     \"MetLife\",\n",
    "#     \"Mettler Toledo\",\n",
    "#     \"Microchip Technology\",\n",
    "#     \"Micron Technology\",\n",
    "#     \"Microsoft\",\n",
    "#     \"Mid-America Apartment Communities\",\n",
    "#     \"Moderna\",\n",
    "#     \"Mondelez International\",\n",
    "#     \"Monster Beverage\",\n",
    "#     \"Moody's Corporation\",\n",
    "#     \"Morgan Stanley\",\n",
    "#     \"Motorola Solutions\",\n",
    "#     \"MSCI\",\n",
    "#     \"Nasdaq\",\n",
    "#     \"NetApp\",\n",
    "#     \"Netflix\",\n",
    "#     \"Newell Brands\",\n",
    "#     \"Newmont\",\n",
    "#     \"News Corp\",\n",
    "#     \"NextEra Energy\",\n",
    "#     \"Nike\",\n",
    "#     \"NiSource\",\n",
    "#     \"Nordson Corporation\",\n",
    "#     \"Norfolk Southern\",\n",
    "#     \"Northern Trust\",\n",
    "#     \"Northrop Grumman\",\n",
    "#     \"Nucor\",\n",
    "#     \"NVIDIA\",\n",
    "#     \"NXP Semiconductors\",\n",
    "#     \"Occidental Petroleum\",\n",
    "#     \"Old Dominion Freight Line\",\n",
    "#     \"Omnicom Group\",\n",
    "#     \"ONEOK\",\n",
    "#     \"Oracle\",\n",
    "#     \"Otis Worldwide\",\n",
    "#     \"Paccar\",\n",
    "#     \"Packaging Corporation of America\",\n",
    "#     \"Paramount Global\",\n",
    "#     \"Parker Hannifin\",\n",
    "#     \"Paychex\",\n",
    "#     \"PayPal\",\n",
    "#     \"PepsiCo\",\n",
    "#     \"Pfizer\",\n",
    "#     \"Philip Morris International\",\n",
    "#     \"Phillips 66\",\n",
    "#     \"PNC Financial Services\",\n",
    "#     \"PPG Industries\",\n",
    "#     \"Procter & Gamble\",\n",
    "#     \"Progressive Corporation\",\n",
    "#     \"Prologis\",\n",
    "#     \"Prudential Financial\",\n",
    "#     \"Public Service Enterprise Group\",\n",
    "#     \"PulteGroup\",\n",
    "#     \"Qualcomm\",\n",
    "#     \"Quanta Services\",\n",
    "#     \"Quest Diagnostics\",\n",
    "#     \"Raymond James\",\n",
    "#     \"Raytheon Technologies\",\n",
    "#     \"Regions Financial\",\n",
    "#     \"Republic Services\",\n",
    "#     \"ResMed\",\n",
    "#     \"Robert Half\",\n",
    "#     \"Rockwell Automation\",\n",
    "#     \"Rollins\",\n",
    "#     \"Roper Technologies\",\n",
    "#     \"Ross Stores\",\n",
    "#     \"S&P Global\",\n",
    "#     \"Salesforce\",\n",
    "#     \"ServiceNow\",\n",
    "#     \"Sherwin-Williams\",\n",
    "#     \"Simon Property Group\",\n",
    "#     \"Skyworks Solutions\",\n",
    "#     \"Snap-on\",\n",
    "#     \"Southern Company\",\n",
    "#     \"Southwest Airlines\",\n",
    "#     \"Stanley Black & Decker\",\n",
    "#     \"State Street Corporation\",\n",
    "#     \"Stryker Corporation\",\n",
    "#     \"T-Mobile US\",\n",
    "#     \"Tapestry\",\n",
    "#     \"Target\",\n",
    "#     \"TE Connectivity\",\n",
    "#     \"Textron\",\n",
    "#     \"The Clorox Company\",\n",
    "#     \"The Hershey Company\",\n",
    "#     \"The Travelers Companies\",\n",
    "#     \"Thermo Fisher Scientific\",\n",
    "#     \"TJX Companies\",\n",
    "#     \"Tractor Supply\",\n",
    "#     \"Trane Technologies\",\n",
    "#     \"TransDigm Group\",\n",
    "#     \"Trowe Price\",\n",
    "#     \"Truist Financial\",\n",
    "#     \"Tyson Foods\",\n",
    "#     \"Union Pacific\",\n",
    "#     \"United Airlines Holdings\",\n",
    "#     \"United Parcel Service\",\n",
    "#     \"United Rentals\",\n",
    "#     \"UnitedHealth Group\",\n",
    "#     \"Valero Energy\",\n",
    "#     \"Verizon\",\n",
    "#     \"Vertex Pharmaceuticals\",\n",
    "#     \"Visa\",\n",
    "#     \"Vulcan Materials\",\n",
    "#     \"Walmart\",\n",
    "#     \"Walt Disney\",\n",
    "#     \"Warner Bros. Discovery\",\n",
    "#     \"Waste Management\",\n",
    "#     \"Wells Fargo\",\n",
    "#     \"Welltower\",\n",
    "#     \"West Pharmaceutical Services\",\n",
    "#     \"Western Digital\",\n",
    "#     \"Weyerhaeuser\",\n",
    "#     \"Whirlpool\",\n",
    "#     \"Williams Companies\",\n",
    "#     \"Xcel Energy\",\n",
    "#     \"Xylem\",\n",
    "#     \"Yum! Brands\",\n",
    "#     \"Zimmer Biomet\",\n",
    "#     \"Zions Bancorp\",\n",
    "#     \"Zoetis\"\n",
    "# ]\n",
    "\n",
    "# # Base question template\n",
    "# question_template = \"What supply chain risks did {company} mention in its 10-K report?\"\n",
    "\n",
    "# # Directory to store outputs (optional)\n",
    "# output_dir = \"sp500_json_outputs\"\n",
    "\n",
    "# import os\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# for company in companies:\n",
    "#     query = question_template.format(company=company)\n",
    "#     context = f\"Analyze the 10-K filing of {company} and summarize its supply chain risks.\"\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# You are a financial analyst. Answer professionally and concisely.\n",
    "# Question: {query}\n",
    "# Context: {context}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "#     print(f\"\\n🔍 Processing {company}...\")\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4o-mini\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#             temperature=0.2,\n",
    "#         )\n",
    "#         answer = response.choices[0].message.content.strip()\n",
    "\n",
    "#         # Save to JSON\n",
    "#         timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#         output = {\n",
    "#             \"company\": company,\n",
    "#             \"query\": query,\n",
    "#             \"response\": answer,\n",
    "#             \"timestamp\": timestamp\n",
    "#         }\n",
    "\n",
    "#         filename = os.path.join(output_dir, f\"{company.replace(' ', '_')}_{timestamp}.json\")\n",
    "#         with open(filename, \"w\") as f:\n",
    "#             json.dump(output, f, indent=4)\n",
    "\n",
    "#         print(f\"✅ Saved {company} → {filename}\")\n",
    "\n",
    "#         # Prevent API overload\n",
    "#         time.sleep(2)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error with {company}: {e}\")\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
